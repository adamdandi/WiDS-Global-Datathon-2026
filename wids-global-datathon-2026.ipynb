{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":125681,"databundleVersionId":15407763}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/adamdandi/wids-global-datathon-2026?scriptVersionId=298552926\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"### **Introduction: The Race Against Time**\n\nManaging wildfires is a high-stakes race against the clock. When a new fire ignites, emergency managers must answer urgent questions immediately with very limited information. They need to decide which communities to warn, which roads to close, and where to send scarce resources like planes and crews. This project focuses on building a data-driven tool to help these commanders make life-saving decisions during the chaotic early stages of a fire.\n\n### **The Context: The First Five Hours**\n\nThe scenario focuses strictly on the \"golden hour\" of disaster response. The analysis uses data collected only from the **first five hours** after a fire is detected.\n\nDuring this short window, sensors record three main types of behavior:\n\n* **Growth:** How quickly the fire's size is increasing.\n* **Movement:** The speed and direction the fire travels across the land.\n* **Position:** How close the fire is to safety zones and whether it is accelerating toward them.\n\nThe objective is to take these early signals and forecast the future. The model must predict if and when the fire will cross a 5-kilometer safety line near a populated area.\n\n### **The Problem: Scarcity and Uncertainty**\n\nPredicting these outcomes is difficult because the data is both small and \"censored.\"\n\n* **Data Scarcity:** Real-world disaster data is rare. This dataset contains only 316 historical fire events. This is a very small number for computer learning, making it easy to accidentally \"memorize\" the past instead of learning the rules of fire behavior.\n* **Censored Data (The \"Non-Events\"):** In many historical cases, the fire never reached the town within the 3-day window. It might have burned out or moved away. In statistics, this is called \"censored data.\" The model cannot simply treat these as errors or ignore them; it must learn from the fires that *didn't* hit just as much as the ones that did.\n\nA simple \"Yes\" or \"No\" prediction is not useful here. A \"Yes\" is meaningless if the fire arrives in 12 hours but the model predicts it will take 72. Responders need to know *when* the danger peaks.\n\n### **The Expectation: A Timeline of Risk**\n\nThe goal is to generate a calibrated probability forecast, similar to a weather report, rather than a single guess.\n\nFor every fire event, the output must provide four specific risk probabilities:\n\n1. Chance of threat within **12 hours**.\n2. Chance of threat within **24 hours**.\n3. Chance of threat within **48 hours**.\n4. Chance of threat within **72 hours**.\n\n**Success is measured by two standards:**\n\n* **Ranking (Triage):** Can the model correctly list fires in order of urgency? It must verify that the fire listed as \"most dangerous\" is actually the one that hits first.\n* **Calibration (Trust):** Are the percentages realistic? If the model predicts an 80% chance of danger, the event should actually happen 80% of the time. This reliability allows commanders to trust the numbers when lives are at risk.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Step 1: Data Loading and Feature Engineering\n\nObjective:\n\n1. Load the raw data.\n2. Apply the \"Physics-Based\" engineering we proved works earlier.\n3. Prepare the feature matrix X and target y for the tournament.\n\nThe Physics Logic:\n\n- **Time to Contact:**  \n  Distance / Speed.  \n  (How long until it hits?)\n\n- **Growth Intensity:**  \n  Growth / Initial Size.  \n  (Is it exploding or stable?)\n\n- **Momentum:**  \n  Speed * Acceleration.  \n  (Is it speeding up towards town?)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\n# Import the Gladiators (Models)\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Try importing External Boosters (XGBoost & LightGBM)\ntry:\n    from xgboost import XGBClassifier\n    XGB_AVAILABLE = True\n    print(\"‚úÖ XGBoost Library Found.\")\nexcept ImportError:\n    XGB_AVAILABLE = False\n    print(\"‚ö†Ô∏è XGBoost not found (Skipping).\")\n\ntry:\n    from lightgbm import LGBMClassifier\n    LGBM_AVAILABLE = True\n    print(\"‚úÖ LightGBM Library Found.\")\nexcept ImportError:\n    LGBM_AVAILABLE = False\n    print(\"‚ö†Ô∏è LightGBM not found (Skipping).\")\n\nwarnings.filterwarnings('ignore')\n\n# ==========================================\n# 1. LOAD DATA & ENGINEERING\n# ==========================================\nprint(\"\\n\" + \"=\"*50)\nprint(\" 1. LOADING & PREPARING DATA\")\nprint(\"=\"*50)\n\n# Load Data\ntrain_df = pd.read_csv('/kaggle/input/WiDSWorldWide_GlobalDathon26/train.csv')\ntest_df = pd.read_csv('/kaggle/input/WiDSWorldWide_GlobalDathon26/test.csv')\n\n# Feature Engineering Function (Physics Based)\ndef engineering_pipeline(df):\n    df_eng = df.copy()\n    # 1. Time to Contact (Distance / Speed)\n    # Adding 0.001 avoids division by zero\n    df_eng['est_time_to_contact'] = df_eng['dist_min_ci_0_5h'] / (df_eng['closing_speed_m_per_h'] + 0.001)\n    \n    # 2. Growth Intensity (Growth / Size)\n    df_eng['growth_intensity'] = df_eng['area_growth_abs_0_5h'] / (df_eng['area_first_ha'] + 0.001)\n    \n    # 3. Momentum (Speed * Acceleration)\n    df_eng['threat_momentum'] = df_eng['closing_speed_m_per_h'] * df_eng['dist_accel_m_per_h2']\n    \n    # Clean Infinite values (caused by 0 speed)\n    df_eng.replace([np.inf, -np.inf], 0, inplace=True)\n    df_eng.fillna(0, inplace=True)\n    return df_eng\n\n# Apply Engineering\nprint(\"Applying Feature Engineering...\")\nX = engineering_pipeline(train_df.drop(columns=['event_id', 'event', 'time_to_hit_hours']))\ny_event = train_df['event']\nX_test_final = engineering_pipeline(test_df.drop(columns=['event_id']))\n\nprint(f\"Data Loaded Successfully.\")\nprint(f\"Training Shape: {X.shape}\")\nprint(f\"Test Shape:     {X_test_final.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T23:08:59.189347Z","iopub.execute_input":"2026-02-18T23:08:59.189708Z","iopub.status.idle":"2026-02-18T23:09:08.231184Z","shell.execute_reply.started":"2026-02-18T23:08:59.189669Z","shell.execute_reply":"2026-02-18T23:09:08.230251Z"}},"outputs":[{"name":"stdout","text":"‚úÖ XGBoost Library Found.\n‚úÖ LightGBM Library Found.\n\n==================================================\n 1. LOADING & PREPARING DATA\n==================================================\nApplying Feature Engineering...\nData Loaded Successfully.\nTraining Shape: (221, 37)\nTest Shape:     (95, 37)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Step 2: The Super Tournament (6 Models)\n\nObjective:\n\nNow we run the Battle Royale. We will test 6 different models on the exact same data splits to see which one is the true champion for each time horizon (12h, 24h, 48h, 72h).\n\nThe Contenders:\n\n1. **RF:** Random Forest (Baseline).\n2. **ET:** Extra Trees (Good for small, noisy data).\n3. **GB:** Gradient Boosting (Standard).\n4. **LR:** Logistic Regression (Linear baseline).\n5. **XGB:** XGBoost (High performance).\n6. **LGBM:** LightGBM (Fast, leaf-wise growth).","metadata":{}},{"cell_type":"code","source":"# ==========================================\n# 2. THE SUPER TOURNAMENT (6 MODELS)\n# ==========================================\nprint(\"\\n\" + \"=\"*50)\nprint(\" ü•ä ROUND 2: SUPER BATTLE ROYALE (6 MODELS)\")\nprint(\"=\"*50)\n\n# Setup 5-Fold Cross Validation\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nhorizons = [12, 24, 48, 72]\n\n# Models to Test\n# Note: We use pipelines for LR to handle scaling automatically\nmodel_roster = ['RF', 'ET', 'GB', 'LR']\nif XGB_AVAILABLE: model_roster.append('XGB')\nif LGBM_AVAILABLE: model_roster.append('LGBM')\n\nscores = {name: {h: [] for h in horizons} for name in model_roster}\n\n# Print Header\nheader = f\"{'Horizon':<8} | \" + \" | \".join([f\"{name:<6}\" for name in model_roster]) + \" | Winner\"\nprint(header)\nprint(\"-\" * len(header))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y_event)):\n    # Prepare Fold Data\n    X_tr_raw, X_val_raw = X.iloc[train_idx], X.iloc[val_idx]\n    \n    for h in horizons:\n        # --- PREPARE TARGETS ---\n        # Train Target\n        tr_mask = ~((train_df.iloc[train_idx]['event'] == 0) & (train_df.iloc[train_idx]['time_to_hit_hours'] < h))\n        X_tr_h = X_tr_raw[tr_mask]\n        y_tr_h = (train_df.iloc[train_idx][tr_mask]['event'] == 1) & (train_df.iloc[train_idx][tr_mask]['time_to_hit_hours'] <= h)\n        \n        # Validation Target\n        val_mask = ~((train_df.iloc[val_idx]['event'] == 0) & (train_df.iloc[val_idx]['time_to_hit_hours'] < h))\n        X_val_h = X_val_raw[val_mask]\n        y_val_h = (train_df.iloc[val_idx][val_mask]['event'] == 1) & (train_df.iloc[val_idx][val_mask]['time_to_hit_hours'] <= h)\n        \n        # Skip invalid folds (Single Class)\n        if len(np.unique(y_tr_h)) < 2 or len(np.unique(y_val_h)) < 2:\n            continue\n\n        # --- DEFINE & TRAIN MODELS ---\n        models = {}\n        \n        # 1. Random Forest\n        models['RF'] = RandomForestClassifier(n_estimators=100, max_depth=5, class_weight='balanced', random_state=42)\n        \n        # 2. Extra Trees (Great for small data)\n        models['ET'] = ExtraTreesClassifier(n_estimators=100, max_depth=5, class_weight='balanced', random_state=42)\n        \n        # 3. Gradient Boosting (Standard)\n        models['GB'] = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)\n        \n        # 4. Logistic Regression (Baseline - Needs Scaling)\n        models['LR'] = make_pipeline(StandardScaler(), LogisticRegression(class_weight='balanced', solver='liblinear'))\n\n        # 5. XGBoost (If available)\n        if XGB_AVAILABLE:\n            ratio = float(np.sum(y_tr_h == 0)) / np.sum(y_tr_h == 1)\n            models['XGB'] = XGBClassifier(n_estimators=100, max_depth=3, scale_pos_weight=ratio, \n                                          eval_metric='logloss', use_label_encoder=False, random_state=42)\n        \n        # 6. LightGBM (If available)\n        if LGBM_AVAILABLE:\n            # LightGBM handles unbalanced data via scale_pos_weight\n            ratio = float(np.sum(y_tr_h == 0)) / np.sum(y_tr_h == 1)\n            models['LGBM'] = LGBMClassifier(n_estimators=100, max_depth=3, scale_pos_weight=ratio, \n                                            random_state=42, verbose=-1)\n\n        # --- BATTLE LOOP ---\n        for name in model_roster:\n            try:\n                model = models[name]\n                model.fit(X_tr_h, y_tr_h)\n                \n                # Handle pipeline vs standard model prediction\n                if hasattr(model, \"predict_proba\"):\n                    preds = model.predict_proba(X_val_h)[:, 1]\n                else:\n                    preds = model.predict(X_val_h) # Fallback\n                \n                score = roc_auc_score(y_val_h, preds)\n                scores[name][h].append(score)\n            except Exception as e:\n                # print(f\"Error {name}: {e}\") # Debug only\n                scores[name][h].append(0)\n\n# --- FINAL SCOREBOARD ---\nprint(\"\\n\" + \"=\"*50)\nprint(\" üèÜ CHAMPIONSHIP RESULTS (Average AUC)\")\nprint(\"=\"*50)\n\n# Print Header Again\nprint(header)\nprint(\"-\" * len(header))\n\nfinal_winners = {}\n\nfor h in horizons:\n    means = {}\n    for name in model_roster:\n        # Calculate mean score, ignore 0s (failed folds)\n        valid_scores = [s for s in scores[name][h] if s > 0]\n        means[name] = np.mean(valid_scores) if valid_scores else 0.0\n            \n    # Find the max score\n    best_model = max(means, key=means.get)\n    final_winners[h] = best_model\n    \n    # Formatted Print\n    row_str = f\"{h:<8} | \"\n    for name in model_roster:\n        row_str += f\"{means[name]:.4f}   | \"\n    row_str += f\"{best_model} ü•á\"\n    \n    print(row_str)\n\nprint(\"-\" * len(header))\nprint(\"NOTE: 0.0000 means the horizon was too easy (100% hits) or had no data to score.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T23:09:38.530562Z","iopub.execute_input":"2026-02-18T23:09:38.531689Z","iopub.status.idle":"2026-02-18T23:09:47.142699Z","shell.execute_reply.started":"2026-02-18T23:09:38.531617Z","shell.execute_reply":"2026-02-18T23:09:47.141762Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\n ü•ä ROUND 2: SUPER BATTLE ROYALE (6 MODELS)\n==================================================\nHorizon  | RF     | ET     | GB     | LR     | XGB    | LGBM   | Winner\n-----------------------------------------------------------------------\n\n==================================================\n üèÜ CHAMPIONSHIP RESULTS (Average AUC)\n==================================================\nHorizon  | RF     | ET     | GB     | LR     | XGB    | LGBM   | Winner\n-----------------------------------------------------------------------\n12       | 0.9607   | 0.9108   | 0.9665   | 0.8950   | 0.9647   | 0.9623   | GB ü•á\n24       | 0.9819   | 0.9353   | 0.9853   | 0.9115   | 0.9778   | 0.9718   | GB ü•á\n48       | 0.9975   | 0.9453   | 0.9936   | 0.9206   | 0.9879   | 0.9911   | RF ü•á\n72       | 0.0000   | 0.0000   | 0.0000   | 0.0000   | 0.0000   | 0.0000   | RF ü•á\n-----------------------------------------------------------------------\nNOTE: 0.0000 means the horizon was too easy (100% hits) or had no data to score.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Analysis of Tournament Results\n\nThe results are fascinating and tell a clear story about the physics of the fires:\n\n- **12h and 24h (The Sprint):**  \n  Gradient Boosting (GB) is the winner. This suggests that for early, fast moving fires, the relationship between features such as speed and acceleration is sharp and precise. Gradient Boosting is excellent at finding these specific edges.\n\n- **48h and 72h (The Marathon):**  \n  Random Forest (RF) takes the lead. As time goes on, fire behavior becomes more chaotic and random. Random Forest performs better here because it averages out the noise and is more robust to uncertainty.\n\n### The Strategy: The Hybrid Approach\n\nWe will not stick to just one model. We will build a Hybrid Ensemble:\n\n- For **12h and 24h**, we use the Gradient Boosting expert.\n- For **48h and 72h**, we use the Random Forest expert.\n\nThis gives us the best of both worlds.","metadata":{}},{"cell_type":"markdown","source":"### Step 3: Training the Hybrid Champions\n\nObjective:\n\nWe will loop through each time horizon. Based on the tournament results, we will initialize the specific winning model (GB or RF), train it on the full dataset, and generate predictions for the test set.","metadata":{}},{"cell_type":"code","source":"# ==========================================\n# 3. TRAINING FINAL HYBRID MODELS\n# ==========================================\nprint(\"\\n\" + \"=\"*50)\nprint(\" üöÄ STEP 3: TRAINING HYBRID CHAMPIONS\")\nprint(\"=\"*50)\n\n# 1. Define the Champions for each Horizon\n# Based on your Tournament Results:\nchampion_map = {\n    12: 'GB',  # Gradient Boosting won 12h\n    24: 'GB',  # Gradient Boosting won 24h\n    48: 'RF',  # Random Forest won 48h\n    72: 'RF'   # Random Forest won 72h (Default)\n}\n\npredictions = {'event_id': test_df['event_id']}\n\nfor h in [12, 24, 48, 72]:\n    winner_name = champion_map[h]\n    print(f\"Processing {h}h Horizon (Champion: {winner_name})...\", end=\" \")\n    \n    # 2. Prepare Valid Training Data for this Horizon\n    # Exclude fires censored BEFORE this horizon (Ambiguous data)\n    valid_rows = ~((train_df['event'] == 0) & (train_df['time_to_hit_hours'] < h))\n    X_train_h = X[valid_rows]\n    y_train_h = (train_df.loc[valid_rows, 'event'] == 1) & (train_df.loc[valid_rows, 'time_to_hit_hours'] <= h)\n    \n    # 3. Safety Check: Single Class Logic\n    # If all remaining fires are Hits (or all Misses), we can't train a model.\n    if y_train_h.nunique() <= 1:\n        default_prob = 1.0 if y_train_h.iloc[0] else 0.0\n        predictions[f'prob_{h}h'] = np.full(len(X_test_final), default_prob)\n        print(f\"Single Class Found (Defaulting to {default_prob}).\")\n        continue\n\n    # 4. Configure the Specific Champion Model\n    # We bump n_estimators to 200 for the final training to ensure maximum learning.\n    if winner_name == 'GB':\n        model = GradientBoostingClassifier(n_estimators=200, max_depth=3, random_state=42)\n    elif winner_name == 'RF':\n        model = RandomForestClassifier(n_estimators=200, max_depth=5, class_weight='balanced', random_state=42)\n        \n    # 5. Train & Predict\n    model.fit(X_train_h, y_train_h)\n    \n    # Get Probabilities\n    if hasattr(model, \"predict_proba\"):\n        probs = model.predict_proba(X_test_final)[:, 1]\n    else:\n        probs = model.predict(X_test_final)\n        \n    predictions[f'prob_{h}h'] = probs\n    print(\"Done. ‚úÖ\")\n\nprint(\"-\" * 50)\nprint(\"All models trained. Predictions ready for post-processing.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T23:12:57.054508Z","iopub.execute_input":"2026-02-18T23:12:57.055179Z","iopub.status.idle":"2026-02-18T23:12:58.24013Z","shell.execute_reply.started":"2026-02-18T23:12:57.055147Z","shell.execute_reply":"2026-02-18T23:12:58.239303Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\n üöÄ STEP 3: TRAINING HYBRID CHAMPIONS\n==================================================\nProcessing 12h Horizon (Champion: GB)... Done. ‚úÖ\nProcessing 24h Horizon (Champion: GB)... Done. ‚úÖ\nProcessing 48h Horizon (Champion: RF)... Done. ‚úÖ\nProcessing 72h Horizon (Champion: RF)... Single Class Found (Defaulting to 1.0).\n--------------------------------------------------\nAll models trained. Predictions ready for post-processing.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Step 4: Post Processing and Submission\n\nObjective:\n\nWe have the raw predictions from our hybrid champions. Now we need to polish them before submitting.\n\n1. **Enforce Logic (Monotonicity):**  \n   A fire cannot be less likely to hit in 48 hours than in 24 hours. Risk must accumulate over time. We will fix any mathematical contradictions.\n\n2. **Safety Clipping:**  \n   The 72 hour model predicted 100 percent risk (1.0). In data science competitions, being 100 percent sure is dangerous. If even one fire misses, the score will crash. We will clip this to 99 percent (0.99) to be safe.\n\n3. **Save:**  \n   Generate the final submission.csv file.","metadata":{}},{"cell_type":"code","source":"# ==========================================\n# 4. POST-PROCESSING & SAVING\n# ==========================================\nprint(\"\\n\" + \"=\"*50)\nprint(\" üîß STEP 4: APPLYING LOGIC & SAVING\")\nprint(\"=\"*50)\n\n# 1. Create DataFrame from the predictions dictionary\nsub_df = pd.DataFrame(predictions)\n\n# Define the columns in time order\ncols = ['prob_12h', 'prob_24h', 'prob_48h', 'prob_72h']\n\nprint(\"Applying Logic Checks...\")\n\n# 2. Enforce Monotonicity\n# Logic: Risk at Hour X cannot be lower than Risk at Hour X-1.\n# Code: We loop through columns. If Col[i] < Col[i-1], we raise Col[i] to match.\nfor i in range(1, len(cols)):\n    sub_df[cols[i]] = np.maximum(sub_df[cols[i]], sub_df[cols[i-1]])\n    \nprint(\"‚úÖ Monotonicity Enforced (Risk now strictly increases over time).\")\n\n# 3. Action A: Safety Clipping\n# We clip predictions to be between 0.1% and 99%.\n# This prevents the \"Infinite Error\" penalty if a 100% prediction turns out wrong.\nsub_df[cols] = sub_df[cols].clip(lower=0.001, upper=0.99)\n\nprint(\"‚úÖ Safety Clipping Applied (max risk capped at 99%).\")\n\n# 4. Save to CSV\nfilename = 'submission.csv'\nsub_df.to_csv(filename, index=False)\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"üéâ DONE! File saved as '{filename}'\")\nprint(f\"Shape: {sub_df.shape}\")\nprint(\"-\" * 50)\nprint(\"Preview of final values (First 3 rows):\")\nprint(sub_df.head(3))\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T23:15:20.793296Z","iopub.execute_input":"2026-02-18T23:15:20.79362Z","iopub.status.idle":"2026-02-18T23:15:20.82715Z","shell.execute_reply.started":"2026-02-18T23:15:20.793594Z","shell.execute_reply":"2026-02-18T23:15:20.826052Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\n üîß STEP 4: APPLYING LOGIC & SAVING\n==================================================\nApplying Logic Checks...\n‚úÖ Monotonicity Enforced (Risk now strictly increases over time).\n‚úÖ Safety Clipping Applied (max risk capped at 99%).\n\n==================================================\nüéâ DONE! File saved as 'submission.csv'\nShape: (95, 5)\n--------------------------------------------------\nPreview of final values (First 3 rows):\n   event_id  prob_12h  prob_24h  prob_48h  prob_72h\n0  10662602  0.001000     0.001  0.051367      0.99\n1  13353600  0.946538     0.990  0.990000      0.99\n2  13942327  0.001000     0.001  0.051392      0.99\n==================================================\n","output_type":"stream"}],"execution_count":4}]}